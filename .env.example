# Backend Configuration
ENV=development
LOG_LEVEL=INFO
API_HOST=0.0.0.0
API_PORT=8000

# Database Configuration
# Option 1: Docker Compose (default)
CHROMA_HOST=localhost
CHROMA_PORT=8001
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Option 2: Local Neo4j Desktop
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=your_local_password

# LLM Configuration
# Option 1: LMStudio (local, recommended for development)
LLM_PROVIDER=lmstudio
LM_STUDIO_API_BASE=http://localhost:1234/v1
LM_STUDIO_MODEL=local-model

# Option 2: OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=your_openai_key_here
# OPENAI_MODEL=gpt-4

# Option 3: Anthropic Claude
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=your_anthropic_key_here

# Option 4: Google Gemini
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=your_gemini_key_here

# Option 5: Ollama (local)
# LLM_PROVIDER=ollama
# OLLAMA_API_BASE=http://localhost:11434
# OLLAMA_MODEL=llama2

# Embedding Configuration (for Phase 4+)
# Embeddings can use a different provider/model than LLM calls
# IMPORTANT: For LMStudio, you MUST set EMBEDDING_MODEL to your embedding model name
EMBEDDING_PROVIDER=lmstudio
EMBEDDING_MODEL=text-embedding-ada-002
# If using OpenAI for embeddings:
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-ada-002
# OPENAI_API_KEY=your_openai_key_here

# Session Configuration
SESSION_TIMEOUT_HOURS=24
API_KEYS=key1,key2,key3

# Frontend Configuration
VITE_API_URL=http://localhost:8000
VITE_WS_URL=ws://localhost:8000/ws
